{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_length: int):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        self.dense_layer = nn.Linear(int(input_length), int(input_length))\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.dense_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_length: int):\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense = nn.Linear(int(input_length), 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.dense(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(max_int: int = 16, batch_size: int = 16, epochs: int = 500):\n",
    "    \n",
    "    input_length = int(math.log(max_int, 2))\n",
    "    \n",
    "    # models\n",
    "    generator = Generator(input_length)\n",
    "    discriminator = Discriminator(input_length)\n",
    "    \n",
    "    # optimizer\n",
    "    generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.01)\n",
    "    discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.01)\n",
    "    \n",
    "    # loss\n",
    "    loss = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # zero gradients on each iterations\n",
    "        generator_optimizer.zero_grad()\n",
    "        \n",
    "        # create noisy input for generator\n",
    "        # need float type instead of int\n",
    "        noise = torch.randint(0, 2, size=(batch_size, input_length)).float()\n",
    "        generated_data = generator(noise)\n",
    "        if epoch % 100 == 0:\n",
    "            print(generated_data)\n",
    "        # Generate examples of even real data\n",
    "        true_labels, true_data = data_generator(max_int, batch_size=batch_size)\n",
    "        true_labels = torch.tensor(true_labels).float()\n",
    "        true_data = torch.tensor(true_data).float()\n",
    "        \n",
    "        # Train the generator\n",
    "        # we invert the labels here and don't train the discriminator because we want the\n",
    "        # generator to make things the discriminator classifies as true\n",
    "        generator_discriminator_out = discriminator(generated_data)\n",
    "        generator_loss = loss(generator_discriminator_out, true_labels) # comparision of discriminators output on generator generated data and true labels\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "        \n",
    "        # now train the discriminator on true/generated data\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        true_discriminator_out = discriminator(true_data)\n",
    "        true_discriminator_loss = loss(true_discriminator_out, true_labels) # comparision of discriminators output on true data to true labels\n",
    "        \n",
    "        # feed the discriminator with detached generated_data tensor\n",
    "        generator_discriminator_out = discriminator(generated_data.detach())\n",
    "        generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros(batch_size))\n",
    "        discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "        \n",
    "    return generator\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4149, 0.4299, 0.4190, 0.5667],\n",
      "        [0.5079, 0.4972, 0.3794, 0.4536],\n",
      "        [0.3149, 0.4404, 0.4101, 0.5856],\n",
      "        [0.3109, 0.3683, 0.3136, 0.5676],\n",
      "        [0.4385, 0.4304, 0.2653, 0.5030]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.1818, 0.1044, 0.8138, 0.4112],\n",
      "        [0.2547, 0.1256, 0.6289, 0.4619],\n",
      "        [0.3872, 0.2922, 0.5355, 0.5998],\n",
      "        [0.1152, 0.0852, 0.8418, 0.4774],\n",
      "        [0.1727, 0.1356, 0.7538, 0.5426]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.7317, 0.7110, 0.1901, 0.1259],\n",
      "        [0.7733, 0.7707, 0.0862, 0.0511],\n",
      "        [0.8671, 0.8322, 0.1143, 0.0358],\n",
      "        [0.8371, 0.7891, 0.0764, 0.0436],\n",
      "        [0.7733, 0.7707, 0.0862, 0.0511]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.1763, 0.7880, 0.8230, 0.0113],\n",
      "        [0.1332, 0.8144, 0.8711, 0.0146],\n",
      "        [0.1765, 0.8255, 0.8641, 0.0130],\n",
      "        [0.0767, 0.8556, 0.8884, 0.0035],\n",
      "        [0.0765, 0.8232, 0.8534, 0.0030]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.6005, 0.2575, 0.6333, 0.1605],\n",
      "        [0.6005, 0.2575, 0.6333, 0.1605],\n",
      "        [0.8166, 0.0516, 0.9534, 0.0014],\n",
      "        [0.8541, 0.0726, 0.9698, 0.0012],\n",
      "        [0.6165, 0.1191, 0.7313, 0.0398]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[8.6842e-01, 4.1670e-01, 1.7179e-01, 2.0678e-02],\n",
      "        [9.5307e-01, 4.3711e-01, 1.1324e-01, 3.0435e-03],\n",
      "        [9.6602e-01, 3.4279e-01, 7.1817e-02, 7.6483e-04],\n",
      "        [8.3173e-01, 3.0449e-01, 1.2878e-01, 2.5098e-02],\n",
      "        [8.6805e-01, 4.4966e-01, 2.4411e-01, 2.4625e-02]],\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([[4.0164e-01, 8.4461e-01, 1.5358e-01, 1.3523e-02],\n",
      "        [2.1455e-01, 9.6044e-01, 3.4981e-02, 3.4623e-04],\n",
      "        [3.1157e-01, 8.5647e-01, 2.0814e-01, 1.7387e-02],\n",
      "        [1.9677e-01, 9.1201e-01, 9.0374e-02, 3.0702e-03],\n",
      "        [1.9758e-01, 9.6574e-01, 5.2810e-02, 4.0926e-04]],\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([[1.1677e-01, 5.8433e-01, 7.0571e-01, 1.0651e-02],\n",
      "        [8.0932e-03, 7.8583e-01, 9.5047e-01, 3.4211e-05],\n",
      "        [8.0932e-03, 7.8583e-01, 9.5047e-01, 3.4211e-05],\n",
      "        [2.5517e-02, 8.0635e-01, 9.3522e-01, 2.1656e-04],\n",
      "        [4.8728e-02, 5.6182e-01, 7.9271e-01, 1.5728e-03]],\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([[9.3309e-02, 2.6034e-02, 9.7155e-01, 1.7583e-04],\n",
      "        [3.0137e-01, 1.2012e-01, 8.4237e-01, 7.7701e-03],\n",
      "        [2.4822e-01, 1.2134e-01, 8.1446e-01, 8.4190e-03],\n",
      "        [1.2178e-01, 4.2259e-02, 8.8954e-01, 1.2611e-03],\n",
      "        [5.9883e-02, 1.2763e-02, 9.8792e-01, 2.2462e-05]],\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([[9.7394e-01, 5.6093e-02, 4.6247e-01, 1.2325e-04],\n",
      "        [9.0119e-01, 1.7723e-01, 4.3768e-01, 6.1077e-03],\n",
      "        [9.1612e-01, 9.2816e-02, 4.1345e-01, 1.1022e-03],\n",
      "        [8.2328e-01, 1.2037e-01, 3.1225e-01, 7.6668e-03],\n",
      "        [9.6066e-01, 1.1098e-01, 4.4200e-01, 7.3681e-04]],\n",
      "       grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "gen = train(16, 5, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.4577e-01, 9.2992e-01, 8.2193e-02, 7.2285e-04],\n",
       "        [9.6216e-01, 9.1944e-01, 6.7002e-02, 5.6124e-04],\n",
       "        [9.7494e-01, 9.4772e-01, 3.5266e-02, 9.0865e-05],\n",
       "        [9.4265e-01, 8.6681e-01, 5.0033e-02, 6.5863e-04],\n",
       "        [9.4265e-01, 8.6681e-01, 5.0033e-02, 6.5863e-04],\n",
       "        [8.8050e-01, 8.2650e-01, 1.1428e-01, 5.2189e-03],\n",
       "        [8.2648e-01, 7.3091e-01, 8.6447e-02, 6.1195e-03],\n",
       "        [9.0408e-01, 8.1749e-01, 1.2981e-01, 4.8974e-03],\n",
       "        [9.7494e-01, 9.4772e-01, 3.5266e-02, 9.0865e-05],\n",
       "        [8.2648e-01, 7.3091e-01, 8.6447e-02, 6.1195e-03]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = torch.randint(0, 2, size=(10, 4)).float() \n",
    "gen(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
